{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2860.49s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pytorch-ignite\n",
      "  Downloading pytorch_ignite-0.4.13-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: torch<3,>=1.3 in ./.local/lib/python3.10/site-packages (from pytorch-ignite) (2.1.2)\n",
      "Requirement already satisfied: packaging in ./.local/lib/python3.10/site-packages (from pytorch-ignite) (23.2)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in ./.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (4.9.0)\n",
      "Requirement already satisfied: sympy in ./.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (1.12)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (3.1.2)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (2023.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in ./.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in ./.local/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3,>=1.3->pytorch-ignite) (12.2.140)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.local/lib/python3.10/site-packages (from jinja2->torch<3,>=1.3->pytorch-ignite) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.local/lib/python3.10/site-packages (from sympy->torch<3,>=1.3->pytorch-ignite) (1.3.0)\n",
      "Downloading pytorch_ignite-0.4.13-py3-none-any.whl (272 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.4/272.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytorch-ignite\n",
      "Successfully installed pytorch-ignite-0.4.13\n"
     ]
    }
   ],
   "source": [
    "# !pip install pytorch-ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# !pip install albumentations==0.4.6\n",
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import ast\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot set memory growth on device when virtual devices configured",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m tf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mset_visible_devices(gpus[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Enable memory growth to allocate GPU memory on an as-needed basis\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperimental\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_memory_growth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgpus\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Set the = activationdesired memory limit (in MB)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m tf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mset_virtual_device_configuration(\n\u001b[1;32m     12\u001b[0m     gpus[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     13\u001b[0m     [tf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mVirtualDeviceConfiguration(memory_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10240\u001b[39m)]\n\u001b[1;32m     14\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/config.py:754\u001b[0m, in \u001b[0;36mset_memory_growth\u001b[0;34m(device, enable)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig.experimental.set_memory_growth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_memory_growth\u001b[39m(device, enable):\n\u001b[1;32m    731\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Set if memory growth should be enabled for a `PhysicalDevice`.\u001b[39;00m\n\u001b[1;32m    732\u001b[0m \n\u001b[1;32m    733\u001b[0m \u001b[38;5;124;03m  If memory growth is enabled for a `PhysicalDevice`, the runtime initialization\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;124;03m    RuntimeError: Runtime is already initialized.\u001b[39;00m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 754\u001b[0m   \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_memory_growth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1771\u001b[0m, in \u001b[0;36mContext.set_memory_growth\u001b[0;34m(self, dev, enable)\u001b[0m\n\u001b[1;32m   1768\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized device: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mrepr\u001b[39m(dev))\n\u001b[1;32m   1770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dev \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_virtual_device_map:\n\u001b[0;32m-> 1771\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1772\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot set memory growth on device when virtual devices configured\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dev\u001b[38;5;241m.\u001b[39mdevice_type \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m dev \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pluggable_devices:\n\u001b[1;32m   1775\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1776\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot set memory growth on non-GPU and non-Pluggable devices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot set memory growth on device when virtual devices configured"
     ]
    }
   ],
   "source": [
    "# GPU 메모리 설정\n",
    "# Set the GPU memory growth option\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to allocate only the first GPU\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        # Enable memory growth to allocate GPU memory on an as-needed basis\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        # Set the = activationdesired memory limit (in MB)\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=10240)]\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# GPU 사용이 가능할 경우, GPU를 사용할 수 있게 함.\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_office_desks_df = pd.read_pickle(\"home_office_desks_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Product_Info</th>\n",
       "      <th>Product_Feature</th>\n",
       "      <th>Product_Text</th>\n",
       "      <th>Img_URL</th>\n",
       "      <th>Product URL</th>\n",
       "      <th>img_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Home &amp; Kitchen, Furniture, Home Office Furnit...</td>\n",
       "      <td>A AIRLLEN Computer Desk 63 Inch Modern Simple ...</td>\n",
       "      <td>$169.99</td>\n",
       "      <td>{'Brand': 'A AIRLLEN', 'Product Dimensions': '...</td>\n",
       "      <td>{'Shape': 'Rectangular', 'Desk design': 'Compu...</td>\n",
       "      <td>【Actual Table Dimension】63 L x 23.62 D x 28.35...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/71I9DrrwH9...</td>\n",
       "      <td>https://www.amazon.com/AIRLLEN-Computer-Workst...</td>\n",
       "      <td>/home/all/imgs/Desks/AAIRLLENComputerDeskInchM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Home &amp; Kitchen, Furniture, Home Office Furnit...</td>\n",
       "      <td>Cubiker Computer Home Office Desk with Drawers...</td>\n",
       "      <td>$79.99</td>\n",
       "      <td>{'Brand': 'Cubiker', 'Product Dimensions': '23...</td>\n",
       "      <td>{'Shape': 'Rectangular', 'Desk design': 'Compu...</td>\n",
       "      <td>Modern Confident Style: Cubiker office compute...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/81tfZZqqGd...</td>\n",
       "      <td>https://www.amazon.com/Cubiker-Computer-Office...</td>\n",
       "      <td>/home/all/imgs/Desks/CubikerComputerHomeOffice...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Home &amp; Kitchen, Furniture, Home Office Furnit...</td>\n",
       "      <td>Rise UP Dual Motor Electric Standing Desk 60x3...</td>\n",
       "      <td>$489.99</td>\n",
       "      <td>{'Brand': 'Uncaged Ergonomics', 'Product Dimen...</td>\n",
       "      <td>{'Shape': 'Rectangular', 'Desk design': 'Compu...</td>\n",
       "      <td>A beautiful, durable, dual-motor electric adju...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/71Weo55ETw...</td>\n",
       "      <td>https://www.amazon.com/Electric-Adjustable-Erg...</td>\n",
       "      <td>/home/all/imgs/Desks/RiseUPDualMotorElectricSt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Home &amp; Kitchen, Furniture, Home Office Furnit...</td>\n",
       "      <td>CubiCubi Computer Desk, 40 inch Home Office De...</td>\n",
       "      <td>$54.99</td>\n",
       "      <td>{'Brand': 'CubiCubi', 'Product Dimensions': '1...</td>\n",
       "      <td>{'Shape': 'Rectangular', 'Desk design': 'Compu...</td>\n",
       "      <td>Modern Simple Style: This computer desk suits ...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/71TfWerRrA...</td>\n",
       "      <td>https://www.amazon.com/Cubicubi-Computer-Offic...</td>\n",
       "      <td>/home/all/imgs/Desks/CubiCubiComputerDeskinchH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[Home &amp; Kitchen, Furniture, Home Office Furnit...</td>\n",
       "      <td>Furologee Computer Desk with Drawer and Power ...</td>\n",
       "      <td>$109.99</td>\n",
       "      <td>{'Brand': 'Furologee', 'Product Dimensions': '...</td>\n",
       "      <td>{'Shape': 'Rectangular', 'Desk design': 'Compu...</td>\n",
       "      <td>[Desk with Charging Station]: The computer des...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/71hSkzjDjd...</td>\n",
       "      <td>https://www.amazon.com/Furologee-Computer-Outl...</td>\n",
       "      <td>/home/all/imgs/Desks/FurologeeComputerDeskwith...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Category  \\\n",
       "1  [Home & Kitchen, Furniture, Home Office Furnit...   \n",
       "2  [Home & Kitchen, Furniture, Home Office Furnit...   \n",
       "3  [Home & Kitchen, Furniture, Home Office Furnit...   \n",
       "4  [Home & Kitchen, Furniture, Home Office Furnit...   \n",
       "8  [Home & Kitchen, Furniture, Home Office Furnit...   \n",
       "\n",
       "                                               Title    Price  \\\n",
       "1  A AIRLLEN Computer Desk 63 Inch Modern Simple ...  $169.99   \n",
       "2  Cubiker Computer Home Office Desk with Drawers...   $79.99   \n",
       "3  Rise UP Dual Motor Electric Standing Desk 60x3...  $489.99   \n",
       "4  CubiCubi Computer Desk, 40 inch Home Office De...   $54.99   \n",
       "8  Furologee Computer Desk with Drawer and Power ...  $109.99   \n",
       "\n",
       "                                        Product_Info  \\\n",
       "1  {'Brand': 'A AIRLLEN', 'Product Dimensions': '...   \n",
       "2  {'Brand': 'Cubiker', 'Product Dimensions': '23...   \n",
       "3  {'Brand': 'Uncaged Ergonomics', 'Product Dimen...   \n",
       "4  {'Brand': 'CubiCubi', 'Product Dimensions': '1...   \n",
       "8  {'Brand': 'Furologee', 'Product Dimensions': '...   \n",
       "\n",
       "                                     Product_Feature  \\\n",
       "1  {'Shape': 'Rectangular', 'Desk design': 'Compu...   \n",
       "2  {'Shape': 'Rectangular', 'Desk design': 'Compu...   \n",
       "3  {'Shape': 'Rectangular', 'Desk design': 'Compu...   \n",
       "4  {'Shape': 'Rectangular', 'Desk design': 'Compu...   \n",
       "8  {'Shape': 'Rectangular', 'Desk design': 'Compu...   \n",
       "\n",
       "                                        Product_Text  \\\n",
       "1  【Actual Table Dimension】63 L x 23.62 D x 28.35...   \n",
       "2  Modern Confident Style: Cubiker office compute...   \n",
       "3  A beautiful, durable, dual-motor electric adju...   \n",
       "4  Modern Simple Style: This computer desk suits ...   \n",
       "8  [Desk with Charging Station]: The computer des...   \n",
       "\n",
       "                                             Img_URL  \\\n",
       "1  https://m.media-amazon.com/images/I/71I9DrrwH9...   \n",
       "2  https://m.media-amazon.com/images/I/81tfZZqqGd...   \n",
       "3  https://m.media-amazon.com/images/I/71Weo55ETw...   \n",
       "4  https://m.media-amazon.com/images/I/71TfWerRrA...   \n",
       "8  https://m.media-amazon.com/images/I/71hSkzjDjd...   \n",
       "\n",
       "                                         Product URL  \\\n",
       "1  https://www.amazon.com/AIRLLEN-Computer-Workst...   \n",
       "2  https://www.amazon.com/Cubiker-Computer-Office...   \n",
       "3  https://www.amazon.com/Electric-Adjustable-Erg...   \n",
       "4  https://www.amazon.com/Cubicubi-Computer-Offic...   \n",
       "8  https://www.amazon.com/Furologee-Computer-Outl...   \n",
       "\n",
       "                                            img_path  \n",
       "1  /home/all/imgs/Desks/AAIRLLENComputerDeskInchM...  \n",
       "2  /home/all/imgs/Desks/CubikerComputerHomeOffice...  \n",
       "3  /home/all/imgs/Desks/RiseUPDualMotorElectricSt...  \n",
       "4  /home/all/imgs/Desks/CubiCubiComputerDeskinchH...  \n",
       "8  /home/all/imgs/Desks/FurologeeComputerDeskwith...  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_office_desks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Category', 'Title', 'Price', 'Product_Info', 'Product_Feature',\n",
       "       'Product_Text', 'Img_URL', 'Product URL', 'img_path'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_office_desks_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\'Brand\\': \\'A AIRLLEN\\', \\'Product Dimensions\\': \\'47\"D x 23\"W x 28.35\"H\\', \\'Color\\': \\'Brown\\', \\'Style\\': \\'Modern\\', \\'Base Material\\': \\'Metal\\', \\'Top Material Type\\': \\'Engineered Wood\\', \\'Finish Type\\': \\'Laminated\\', \\'Special Feature\\': \\'Adjustable\\', \\'Room Type\\': \\'Office\\', \\'Recommended Uses For Product\\': \\'Working, Writing, Gaming\\'}'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_office_desks_df.iloc[0,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2559, 9)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_office_desks_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데이터 준비\n",
    "import os\n",
    "import json  \n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "# 스타일 확인\n",
    "styles = ['Classic', 'Contemporary', 'Country', 'Minimalism', 'Modern', 'Unique', 'Urban']\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "class HomeOfficeDeskDataset(Dataset):\n",
    "    def __init__(self, dataframe, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (DataFrame): 'home_office_desks_df' 데이터프레임.\n",
    "            root_dir (string): 이미지 파일이 저장된 폴더의 경로.\n",
    "            transform (callable, optional): 적용할 변환 (예: 데이터 증강).\n",
    "        \"\"\"\n",
    "        self.desk_df = dataframe\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.desk_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.desk_df.iloc[idx, 8])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # 문자열을 딕셔너리로 변환\n",
    "        product_info_str = self.desk_df.iloc[idx, 3]\n",
    "        product_info = eval(product_info_str)\n",
    "        style = product_info['Style'].capitalize()\n",
    "\n",
    "        label = styles.index(style)  # 'styles' 리스트에서 해당 스타일의 인덱스를 라벨로 사용\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "dataset = HomeOfficeDeskDataset(\n",
    "    dataframe=home_office_desks_df,\n",
    "    root_dir='/home/all/imgs/Desks',\n",
    "    transform=transform\n",
    ")\n",
    "data_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'Modern, contemporary' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m num_samples_to_check \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (images, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m num_samples_to_check:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[69], line 46\u001b[0m, in \u001b[0;36mHomeOfficeDeskDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     43\u001b[0m product_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(product_info_str)\n\u001b[1;32m     44\u001b[0m style \u001b[38;5;241m=\u001b[39m product_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStyle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcapitalize()\n\u001b[0;32m---> 46\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[43mstyles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 'styles' 리스트에서 해당 스타일의 인덱스를 라벨로 사용\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
      "\u001b[0;31mValueError\u001b[0m: 'Modern, contemporary' is not in list"
     ]
    }
   ],
   "source": [
    "num_samples_to_check = 5\n",
    "for i, (images, labels) in enumerate(data_loader):\n",
    "    if i >= num_samples_to_check:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.HomeOfficeDeskDataset object at 0x7efc31c097b0>\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Brand': 'A AIRLLEN', 'Product Dimensions': '47\"D x 23\"W x 28.35\"H', 'Color': 'Brown', 'Style': 'Modern', 'Base Material': 'Metal', 'Top Material Type': 'Engineered Wood', 'Finish Type': 'Laminated', 'Special Feature': 'Adjustable', 'Room Type': 'Office', 'Recommended Uses For Product': 'Working, Writing, Gaming'}\n"
     ]
    }
   ],
   "source": [
    "# 특정 인덱스(예: 0번 인덱스)의 값을 출력해 확인\n",
    "sample_data = home_office_desks_df.iloc[0, 3]\n",
    "print(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 모델 변경\n",
    "import torch.nn as nn\n",
    "# 기존 unet\n",
    "class UNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.contracting_11 = self.conv_block(in_channels=3, out_channels=64)\n",
    "        self.contracting_12 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.contracting_21 = self.conv_block(in_channels=64, out_channels=128)\n",
    "        self.contracting_22 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.contracting_31 = self.conv_block(in_channels=128, out_channels=256)\n",
    "        self.contracting_32 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.contracting_41 = self.conv_block(in_channels=256, out_channels=512)\n",
    "        self.contracting_42 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.middle = self.conv_block(in_channels=512, out_channels=1024)\n",
    "        self.expansive_11 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.expansive_12 = self.conv_block(in_channels=1024, out_channels=512)\n",
    "        self.expansive_21 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.expansive_22 = self.conv_block(in_channels=512, out_channels=256)\n",
    "        self.expansive_31 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.expansive_32 = self.conv_block(in_channels=256, out_channels=128)\n",
    "        self.expansive_41 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.expansive_42 = self.conv_block(in_channels=128, out_channels=64)\n",
    "        self.output = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "\t    # 1x1 convolution layer 추가\n",
    "        self.output1 = nn.Conv2d(in_channels=64, out_channels=num_classes, kernel_size=1, stride=1, padding=1)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        block = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.BatchNorm2d(num_features=out_channels),\n",
    "                                    nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.BatchNorm2d(num_features=out_channels))\n",
    "        return block\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "# 수정할 부분\n",
    "\n",
    "\n",
    "class ModifiedUNet(UNet):  # 기존 UNet 클래스를 상속\n",
    "    def __init__(self, num_classes):\n",
    "        super(ModifiedUNet, self).__init__(num_classes)\n",
    "        self.output = nn.Conv2d(64, num_classes, kernel_size=1)  # 1x1 컨볼루션으로 클래스 수 조정\n",
    "\n",
    "    def forward(self, X):\n",
    "        contracting_11_out = self.contracting_11(X) # [-1, 64, 256, 256]\n",
    "        contracting_12_out = self.contracting_12(contracting_11_out) # [-1, 64, 128, 128]\n",
    "        contracting_21_out = self.contracting_21(contracting_12_out) # [-1, 128, 128, 128]\n",
    "        contracting_22_out = self.contracting_22(contracting_21_out) # [-1, 128, 64, 64]\n",
    "        contracting_31_out = self.contracting_31(contracting_22_out) # [-1, 256, 64, 64]\n",
    "        contracting_32_out = self.contracting_32(contracting_31_out) # [-1, 256, 32, 32]\n",
    "        contracting_41_out = self.contracting_41(contracting_32_out) # [-1, 512, 32, 32]\n",
    "        contracting_42_out = self.contracting_42(contracting_41_out) # [-1, 512, 16, 16]\n",
    "        middle_out = self.middle(contracting_42_out) # [-1, 1024, 16, 16]\n",
    "        expansive_11_out = self.expansive_11(middle_out) # [-1, 512, 32, 32]\n",
    "        expansive_12_out = self.expansive_12(torch.cat((expansive_11_out, contracting_41_out), dim=1)) # [-1, 1024, 32, 32] -> [-1, 512, 32, 32]\n",
    "        expansive_21_out = self.expansive_21(expansive_12_out) # [-1, 256, 64, 64]\n",
    "        expansive_22_out = self.expansive_22(torch.cat((expansive_21_out, contracting_31_out), dim=1)) # [-1, 512, 64, 64] -> [-1, 256, 64, 64]\n",
    "        expansive_31_out = self.expansive_31(expansive_22_out) # [-1, 128, 128, 128]\n",
    "        expansive_32_out = self.expansive_32(torch.cat((expansive_31_out, contracting_21_out), dim=1)) # [-1, 256, 128, 128] -> [-1, 128, 128, 128]\n",
    "        expansive_41_out = self.expansive_41(expansive_32_out) # [-1, 64, 256, 256]\n",
    "        expansive_42_out = self.expansive_42(torch.cat((expansive_41_out, contracting_11_out), dim=1)) # [-1, 128, 256, 256] -> [-1, 64, 256, 256]\n",
    "        output_out = self.output(expansive_42_out) # [-1, 64, 256, 256] -> [-1, 64, 256, 256]\n",
    "        output_out1 = self.output(output_out) # [-1, num_classes, 256, 256]\n",
    "        output_out1 = self.output(output_out1)\n",
    "        \n",
    "        return output_out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "shape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     60\u001b[0m     epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m     62\u001b[0m         images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     63\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[24], line 41\u001b[0m, in \u001b[0;36mHomeOfficeDeskDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     37\u001b[0m style \u001b[38;5;241m=\u001b[39m product_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStyle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m     39\u001b[0m label \u001b[38;5;241m=\u001b[39m styles\u001b[38;5;241m.\u001b[39mindex(style)\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# 이미지 크기 및 채널 수 확인\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m     45\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/PIL/Image.py:519\u001b[0m, in \u001b[0;36mImage.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    512\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage categories are deprecated and will be removed in Pillow 10 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(2023-07-01). Use is_animated instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    515\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m    516\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    517\u001b[0m     )\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_category\n\u001b[0;32m--> 519\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(name)\n",
      "\u001b[0;31mAttributeError\u001b[0m: shape"
     ]
    }
   ],
   "source": [
    "# 3. 훈련\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from ignite.handlers import EarlyStopping\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "변환_설정 = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # 이미지 크기를 256x256으로 조정\n",
    "    transforms.ToTensor(),          # 이미지를 PyTorch 텐서로 변환\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 정규화\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# 데이터셋 및 데이터 로더 설정\n",
    "dataset = HomeOfficeDeskDataset(home_office_desks_df, '이미지_폴더_경로', transform=변환_설정)\n",
    "data_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# 모델, 손실 함수, 옵티마이저 설정\n",
    "model = ModifiedUNet(num_classes=len(styles)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Early stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose  # 여기에 verbose 인자 추가\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''모델 저장하는 코드 (옵션)'''\n",
    "        pass\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5, verbose=True)\n",
    "\n",
    "epochs = 25  # 모델을 25번의 에폭(epoch) 동안 훈련시킬 설정\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for images, labels in data_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    early_stopping(epoch_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_score, recall_score\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 평가 데이터셋에서 예측\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# ...\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(\u001b[43my_true\u001b[49m, y_pred)\n\u001b[1;32m      8\u001b[0m precision \u001b[38;5;241m=\u001b[39m precision_score(y_true, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m recall \u001b[38;5;241m=\u001b[39m recall_score(y_true, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_true' is not defined"
     ]
    }
   ],
   "source": [
    "# 4. 평가\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "# 평가 데이터셋에서 예측\n",
    "# ...\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='macro')\n",
    "recall = recall_score(y_true, y_pred, average='macro')\n",
    "\n",
    "print(f'Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "# from unet_model import UNet  # U-Net 모델 아키텍처가 정의된 가정된 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/all/imgs/Desks/AAIRLLENComputerDeskInchModernSimpleSturdyNotebookWritingDeskStudyOfficeTableforHomeOfficeDinningTableWorkstationDeskEscritoriodeOrdenador.jpg'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_office_desks_df.iloc[0,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/myeong/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/myeong/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'dec2_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m unet_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 56\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43munet_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# 결과 시각화\u001b[39;00m\n\u001b[1;32m     59\u001b[0m predicted_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(prediction)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 75\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     71\u001b[0m         pool1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool1(x2)\n\u001b[1;32m     73\u001b[0m \t\t\t\u001b[38;5;66;03m# 생략\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m         unpool1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munpool1(\u001b[43mdec2_1\u001b[49m)\n\u001b[1;32m     76\u001b[0m         cat1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((unpool1, x2), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     77\u001b[0m         dec1_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec1_2(cat1)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dec2_1' is not defined"
     ]
    }
   ],
   "source": [
    "# 이거 수정하나다 끝\n",
    "model_path = '/home/myeong/unet_model.pth'  \n",
    "unet_model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# 이미지 로드 함수 정의\n",
    "def load_image(image_path):\n",
    "    # 이미지 파일이 존재하는지 확인\n",
    "    if not os.path.exists(image_path):\n",
    "        raise FileNotFoundError(f\"Image file not found: {image_path}\")\n",
    "    image = cv2.imread(image_path)\n",
    "    return image\n",
    "\n",
    "# 실제 이미지 로드\n",
    "image_path = '/home/all/imgs/Desks/AAIRLLENComputerDeskInchModernSimpleSturdyNotebookWritingDeskStudyOfficeTableforHomeOfficeDinningTableWorkstationDeskEscritoriodeOrdenador.jpg'\n",
    "image = load_image(image_path)\n",
    "\n",
    "# 초기 마스크와 배경/전경 모델을 생성\n",
    "mask = np.zeros(image.shape[:2], np.uint8)\n",
    "bgdModel = np.zeros((1, 65), np.float64)\n",
    "fgdModel = np.zeros((1, 65), np.float64)\n",
    "\n",
    "# Faster R-CNN 모델 불러오기\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# 이미지를 텐서로 변환\n",
    "tensor_image = F.to_tensor(image)\n",
    "\n",
    "# 이미지에서 객체 탐지\n",
    "with torch.no_grad():\n",
    "    prediction = model([tensor_image])[0]\n",
    "\n",
    "# 탐지된 객체 중 하나 선택 (예: 첫 번째 객체)\n",
    "box = prediction[\"boxes\"][0].cpu().numpy()  # (x_min, y_min, x_max, y_max)\n",
    "rect = (int(box[0]), int(box[1]), int(box[2] - box[0]), int(box[3] - box[1]))\n",
    "\n",
    "# GrabCut 실행\n",
    "cv2.grabCut(image, mask, rect, bgdModel, fgdModel, 5, cv2.GC_INIT_WITH_RECT)\n",
    "\n",
    "# 배경인 픽셀은 0, 그 외에는 1로 설정하여 최종 마스크를 생성\n",
    "mask2 = np.where((mask == 2)|(mask == 0), 0, 1).astype('uint8')\n",
    "grabcut_image = image * mask2[:, :, np.newaxis]\n",
    "\n",
    "# 결과 확인\n",
    "plt.imshow(cv2.cvtColor(grabcut_image, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"GrabCut Mask\")\n",
    "plt.show()\n",
    "\n",
    "# U-Net 모델 로드\n",
    "unet_model = UNet()\n",
    "unet_model.load_state_dict(torch.load('unet_model.pth'))\n",
    "\n",
    "# 세그멘테이션 수행\n",
    "unet_model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = unet_model(tensor_image.unsqueeze(0))\n",
    "\n",
    "# 결과 시각화\n",
    "predicted_mask = torch.sigmoid(prediction).squeeze(0)\n",
    "predicted_mask = predicted_mask.cpu().numpy()\n",
    "plt.imshow(predicted_mask, cmap='gray')\n",
    "plt.title(\"Segmentation Result\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Image file not found: \\home\\all\\imgs\\Desks\\AAIRLLENComputerDeskInchModernSimpleSturdyNotebookWritingDeskStudyOfficeTableforHomeOfficeDinningTableWorkstationDeskEscritoriodeOrdenador.jpg",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 실제 이미지 로드\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mload_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# 초기 마스크와 배경/전경 모델을 생성\u001b[39;00m\n\u001b[1;32m     21\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(image\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m], np\u001b[38;5;241m.\u001b[39muint8)\n",
      "Cell \u001b[0;32mIn[36], line 8\u001b[0m, in \u001b[0;36mload_image\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_image\u001b[39m(image_path):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# 이미지 파일이 존재하는지 확인\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(image_path):\n\u001b[0;32m----> 8\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage file not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m     image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m     transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([transforms\u001b[38;5;241m.\u001b[39mToTensor()])\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Image file not found: \\home\\all\\imgs\\Desks\\AAIRLLENComputerDeskInchModernSimpleSturdyNotebookWritingDeskStudyOfficeTableforHomeOfficeDinningTableWorkstationDeskEscritoriodeOrdenador.jpg"
     ]
    }
   ],
   "source": [
    "# # 데이터프레임에서 실제 이미지 파일 경로를 가져옵니다.\n",
    "# image_path = '/home/all/imgs/Desks/AAIRLLENComputerDeskInchModernSimpleSturdyNotebookWritingDeskStudyOfficeTableforHomeOfficeDinningTableWorkstationDeskEscritoriodeOrdenador.jpg'\n",
    "\n",
    "# # 이미지 로드 함수 정의\n",
    "# def load_image(image_path):\n",
    "#     # 이미지 파일이 존재하는지 확인\n",
    "#     if not os.path.exists(image_path):\n",
    "#         raise FileNotFoundError(f\"Image file not found: {image_path}\")\n",
    "    \n",
    "#     image = Image.open(image_path).convert(\"RGB\")\n",
    "#     transform = transforms.Compose([transforms.ToTensor()])\n",
    "#     image = transform(image)\n",
    "#     return image\n",
    "\n",
    "# # 실제 이미지 로드\n",
    "# image = load_image(image_path)\n",
    "\n",
    "\n",
    "\n",
    "# # 초기 마스크와 배경/전경 모델을 생성\n",
    "# mask = np.zeros(image.shape[:2], np.uint8)\n",
    "# bgdModel = np.zeros((1, 65), np.float64)\n",
    "# fgdModel = np.zeros((1, 65), np.float64)\n",
    "\n",
    "# # rect\n",
    "# # 이미지를 불러오고, 모델에 맞게 전처리하는 함수\n",
    "# def load_image(image_path):\n",
    "#     image = Image.open(image_path).convert(\"RGB\")\n",
    "#     transform = transforms.Compose([transforms.ToTensor()])\n",
    "#     image = transform(image)\n",
    "#     return image\n",
    "\n",
    "# # 이미지 로드\n",
    "# image_path = \"path_to_your_image.jpg\"\n",
    "# image = load_image(image_path)\n",
    "\n",
    "# # YOLO 모델 불러오기 (여기서는 Faster R-CNN을 사용한 예시입니다)\n",
    "# model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "# model.eval()\n",
    "\n",
    "# # 이미지에서 객체 탐지\n",
    "# with torch.no_grad():\n",
    "#     prediction = model([image])\n",
    "\n",
    "# # 탐지된 객체 중 하나 선택 (예: 첫 번째 객체)\n",
    "# boxes = prediction[0][\"boxes\"]\n",
    "# labels = prediction[0][\"labels\"]\n",
    "# scores = prediction[0][\"scores\"]\n",
    "\n",
    "# # 가장 확률이 높은 객체의 경계 상자 선택\n",
    "# # 이 부분은 필요에 따라 수정할 수 있습니다.\n",
    "# box = boxes[0].cpu().numpy()  # (x_min, y_min, x_max, y_max)\n",
    "# rect = (int(box[0]), int(box[1]), int(box[2] - box[0]), int(box[3] - box[1]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # GrabCut 실행\n",
    "# cv2.grabCut(image, mask, rect, bgdModel, fgdModel, 5, cv2.GC_INIT_WITH_RECT)\n",
    "\n",
    "# # 배경인 픽셀은 0, 그 외에는 1로 설정하여 최종 마스크를 생성\n",
    "# mask2 = np.where((mask == 2)|(mask == 0), 0, 1).astype('uint8')\n",
    "# image = image * mask2[:, :, np.newaxis]\n",
    "\n",
    "# # 결과 확인\n",
    "# cv2.imshow(\"GrabCut Mask\", image)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "# # U-Net 모델 로드 (미리 훈련된 가정하에)\n",
    "# unet_model = UNet()\n",
    "# unet_model.load_state_dict(torch.load('unet_model.pth'))\n",
    "\n",
    "# # 이미지를 U-Net 모델에 적합한 형태로 변환\n",
    "# tensor_image = transforms.ToTensor()(image).unsqueeze(0)\n",
    "\n",
    "# # 세그멘테이션 수행\n",
    "# unet_model.eval()  # 모델을 평가 모드로 설정\n",
    "# with torch.no_grad():\n",
    "#     prediction = unet_model(tensor_image)\n",
    "\n",
    "# # 결과 시각화\n",
    "# predicted_mask = torch.sigmoid(prediction).squeeze(0)\n",
    "# predicted_mask = predicted_mask.cpu().numpy()\n",
    "# plt.imshow(predicted_mask, cmap='gray')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
