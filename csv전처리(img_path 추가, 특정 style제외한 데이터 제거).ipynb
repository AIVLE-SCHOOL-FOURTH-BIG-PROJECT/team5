{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4bfe29b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-21 09:34:00.339922: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-21 09:34:00.362439: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-21 09:34:00.362458: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-21 09:34:00.363026: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-21 09:34:00.366504: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-21 09:34:00.801259: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 13603148568566507822\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1250164736\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 15507783086898938361\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-21 09:34:01.238498: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-21 09:34:01.257262: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-21 09:34:01.257419: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-21 09:34:01.374246: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-21 09:34:01.374343: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-21 09:34:01.374416: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-21 09:34:01.374473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 1192 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "# GPU 연결 확인\n",
    "import tensorflow as tf \n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137a5f27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ce3c7fd",
   "metadata": {},
   "source": [
    "**디코딩**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a2f8731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os, csv, re\n",
    "import pandas as pd\n",
    "import glob  # 이 부분을 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4cfc9678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport pandas as pd\\nimport os\\nimport re\\n\\n# removing metacharacters in product title\\ndef sanitize_filename(filename):\\n    # Keep only alphabets (uppercase and lowercase)\\n    return re.sub(r\\'[^a-zA-Z]\\', \\'\\', filename)\\n\\ndef get_image_path(title):\\n    # Sanitize the title to create a valid filename, trimming to 200 characters if necessary\\n    sanitized_title = sanitize_filename(title[:200])\\n\\n    # Construct the path to the image file\\n    file_path = f\"D:\\\\빅프 크롤링2\\\\imgs\\\\Desks\\\\{sanitized_title}.jpg\"  # Assuming the image format is jpg\\n\\n    # Check if the file exists\\n    if os.path.exists(file_path):\\n        return file_path\\n    else:\\n        return \"File not found.\"\\n\\n# Read the CSV file\\ndf = pd.read_csv(\\'D:\\\\빅프 크롤링2\\\\product_infos\\\\Desks_product_infos.csv\\')\\n\\n# Create a new column \\'path\\' in the dataframe\\ndf[\\'path\\'] = df[\\'Title\\'].apply(get_image_path)\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 기존 코드\n",
    "'''\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# removing metacharacters in product title\n",
    "def sanitize_filename(filename):\n",
    "    # Keep only alphabets (uppercase and lowercase)\n",
    "    return re.sub(r'[^a-zA-Z]', '', filename)\n",
    "\n",
    "def get_image_path(title):\n",
    "    # Sanitize the title to create a valid filename, trimming to 200 characters if necessary\n",
    "    sanitized_title = sanitize_filename(title[:200])\n",
    "\n",
    "    # Construct the path to the image file\n",
    "    file_path = f\"D:\\\\빅프 크롤링2\\\\imgs\\\\Desks\\\\{sanitized_title}.jpg\"  # Assuming the image format is jpg\n",
    "\n",
    "    # Check if the file exists\n",
    "    if os.path.exists(file_path):\n",
    "        return file_path\n",
    "    else:\n",
    "        return \"File not found.\"\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('D:\\\\빅프 크롤링2\\\\product_infos\\\\Desks_product_infos.csv')\n",
    "\n",
    "# Create a new column 'path' in the dataframe\n",
    "df['path'] = df['Title'].apply(get_image_path)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af734e67",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\user\\\\Desktop\\\\big_project_crawling2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m test_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124muser\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mbig_project_crawling2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m디렉토리 내 파일:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_path\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\user\\\\Desktop\\\\big_project_crawling2'"
     ]
    }
   ],
   "source": [
    "test_path = 'C:\\\\Users\\\\user\\\\Desktop\\\\big_project_crawling2'\n",
    "print(\"디렉토리 내 파일:\", os.listdir(test_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99294edf",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\user\\\\Desktop\\\\big_project_crawling2\\\\product_infos\\\\'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m test_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124muser\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mbig_project_crawling2\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mproduct_infos\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Test if the directory is accessible and list files\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiles in directory:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Test if glob can find CSV files\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m csv_file \u001b[38;5;129;01min\u001b[39;00m glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(test_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\user\\\\Desktop\\\\big_project_crawling2\\\\product_infos\\\\'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Use an absolute path for testing\n",
    "test_path = 'C:\\\\Users\\\\user\\\\Desktop\\\\big_project_crawling2\\\\product_infos\\\\'\n",
    "\n",
    "# Test if the directory is accessible and list files\n",
    "print(\"Files in directory:\", os.listdir(test_path))\n",
    "\n",
    "# Test if glob can find CSV files\n",
    "for csv_file in glob.glob(os.path.join(test_path, '*.csv')):\n",
    "    print(f\"Processing file: {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0e1ef0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mconcat(all_dataframes, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# 모든 데이터를 하나의 데이터프레임으로 합치기\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_all_csv_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# 결과 확인\u001b[39;00m\n\u001b[1;32m     34\u001b[0m combined_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "Cell \u001b[0;32mIn[9], line 28\u001b[0m, in \u001b[0;36mprocess_all_csv_files\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_path\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m title: get_image_path(title, category))\n\u001b[1;32m     27\u001b[0m     all_dataframes\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_dataframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/reshape/concat.py:380\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    378\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 380\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/reshape/concat.py:443\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[0;32m--> 443\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[1;32m    446\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/reshape/concat.py:505\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[0;34m(self, objs, keys)\u001b[0m\n\u001b[1;32m    502\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 505\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    508\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import glob  # 이 부분을 추가\n",
    "\n",
    "# 기본 경로 설정(01-4.Crawling_integrated를 넣었던 폴더)\n",
    "base_path = 'C:\\\\Users\\\\user\\\\Desktop\\\\big_project_crawling2\\\\'\n",
    "\n",
    "# 파일 이름을 안전하게 만드는 함수\n",
    "def sanitize_filename(filename):\n",
    "    return re.sub(r'[^a-zA-Z]', '', filename)\n",
    "\n",
    "# 이미지 경로를 가져오는 함수\n",
    "def get_image_path(title, category):\n",
    "    sanitized_title = sanitize_filename(title[:200])\n",
    "    file_path = os.path.join(base_path, 'imgs', category, f\"{sanitized_title}.jpg\")\n",
    "    return file_path if os.path.exists(file_path) else \"File not found.\"\n",
    "\n",
    "# 모든 CSV 파일을 처리하고 하나의 데이터프레임으로 합치는 함수\n",
    "def process_all_csv_files():\n",
    "    all_dataframes = []\n",
    "    for csv_file in glob.glob(os.path.join(base_path, 'product_infos', '*.csv')):\n",
    "        category = os.path.basename(csv_file).replace('_product_infos.csv', '')\n",
    "        df = pd.read_csv(csv_file)\n",
    "        df['img_path'] = df['Title'].apply(lambda title: get_image_path(title, category))\n",
    "        all_dataframes.append(df)\n",
    "    return pd.concat(all_dataframes, ignore_index=True)\n",
    "\n",
    "# 모든 데이터를 하나의 데이터프레임으로 합치기\n",
    "combined_df = process_all_csv_files()\n",
    "\n",
    "# 결과 확인\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0a28f5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv로 확인\n",
    "# combined_df.to_csv('C:\\\\Users\\\\Hong_PC\\\\Desktop\\\\combined_df.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "32220131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path에 주소가 없는 경우(클롤링이 실패한 사례)\n",
    "combined_df = combined_df[combined_df['img_path'] != 'File not found.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "95d78fb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유니크한 'Style'의 개수: 103\n",
      "스타일 목록: {'Vintage, Antique, Contemporary', 'K Shape Desk', 'Classic, Industrial', 'Portable', '工业', 'Japanese', 'Contemporary', 'Computer Desk with Coffee Table', 'Modern', 'gaming', 'Gaming desk', 'Traditional', 'Cottage', 'Wide', 'Single Drawer', 'Gray/66 inch Gray Modern', 'Wright', 'Desk with Storage Bin', 'Glass Top', 'Desk', 'Modern Industrial', 'computer desk gaming desk', '波希米亚', 'Right Handed', 'Tropical', 'vintage', 'Spectrum', 'EUREKA ERGONOMIC 65\" Large Electric Height Adjustable', 'L Desk', 'Office', 'Novel', 'No Drawer', 'Foldable', 'Laurel', 'MINIMALIST AND PRACTICAL', 'Industrial,Rustic', 'L-Shaped Computer Desk', '北歐風', '极简主义', '/', 'L Desk with Lift Top', 'traditional', 'Art Deco', 'Modern mid century', 'not_applicable', 'CONTEMPORARY, INDUSTRIAL, MODERN', 'Silver Base', 'Office, Gaming', 'Rustic, modern', 'Black 55\"', 'Country Rustic', 'Minimalist', 'CONTEMPORARY, MODERN', 'Retro', 'Minimalist,Modern', 'With Drawer', 'Modern,simple', 'Computer Desk', 'Floor shelf', 'Contemprary, Modern', 'Gaming Desk', 'computer desk', 'Modern simplicity', 'Nordic Style', 'Commercial', 'Modern/Contemporary', 'Morden', 'Antique', 'Furologee 66\" L Shaped Computer Desk & Printer Stand', 'Scandinavian', 'L Shape', 'Industrial', 'Modern,Contemporary', 'BON AUGURE Computer Desk with Corner shelving unit', 'Eclectic', 'Casual', '60\"W x 30\"D', 'Gaming', 'Country', 'Mid-Century Modern', 'Rustic', 'other', 'Regular Desk', 'Farmhouse', 'L Shaped', 'Transitional', 'Simple', 'L-desk', '19 Inch', '现代', 'FATORRI L Shaped Computer Desk and 5 Tier Bookshelf', 'Classic', 'Stsnding Desk', 'Victorian', 'Farmhouse/Cottage', 'Vintage', 'American', 'Gaming Entertainment', 'Computer Table', 'minimalism', 'White', 'GIKPAL Blakc and White Computer Desk with Drawers', 'Double Drawer & Shelf'}\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "# 'Style' 값을 추출하고 유니크한 값들 찾기\n",
    "styles = []\n",
    "for info in combined_df['Product_Info']:\n",
    "    try:\n",
    "        product_info = ast.literal_eval(info)  # 문자열을 딕셔너리로 변환\n",
    "        if 'Style' in product_info:\n",
    "            styles.append(product_info['Style'])\n",
    "    except ValueError:\n",
    "        continue  # 변환 중 오류 발생 시 무시\n",
    "\n",
    "unique_styles = set(styles)  # 중복 제거\n",
    "style_count = len(unique_styles)  # 개수 계산\n",
    "\n",
    "print(f\"유니크한 'Style'의 개수: {style_count}\")\n",
    "print(\"스타일 목록:\", unique_styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5a7a0833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2559, 8)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 특정 스타일을 남기기전 데이터\n",
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e9b0bdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 올바른 스타일 목록 정의 (여기에 필요한 스타일 추가)\n",
    "valid_styles = ['modern', 'contemporary', 'traditional', 'vintage', 'rustic', 'industrial', 'minimalist', 'scandinavian', 'classic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "852ad331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1914, 8)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 특정 스타일만 남기긴 데이터\n",
    "def is_valid_style(info):\n",
    "    try:\n",
    "        product_info = ast.literal_eval(info)\n",
    "        if 'Style' in product_info:\n",
    "            style = product_info['Style'].lower()  # 소문자로 변환\n",
    "            return any(valid_style in style for valid_style in valid_styles)\n",
    "    except ValueError:\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "# 올바른 스타일을 포함하는 행만 필터링\n",
    "combined_df = combined_df[combined_df['Product_Info'].apply(is_valid_style)]\n",
    "\n",
    "# 필터링된 데이터프레임 저장\n",
    "combined_df.shape # 대략 20% 손실"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "84b2ede7",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_pickle(\"combined_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d79203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44db9ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65924a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
