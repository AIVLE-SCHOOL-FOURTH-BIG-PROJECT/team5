{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-26 14:30:48.411290: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-26 14:30:48.434146: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-26 14:30:48.434166: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-26 14:30:48.434738: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-26 14:30:48.438627: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-26 14:30:48.876154: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or '3' to suppress most of the logs\n",
    "tf.get_logger().setLevel('WARNING')  # Adjust logging level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('images64.npy')\n",
    "y = np.load('styles64.npy')\n",
    "# X and y are now numpy arrays containing your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Classic', 'Contemporary', 'Country', 'Minimalism', 'Modern',\n",
       "       'Unique', 'Urban'], dtype='<U12')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Assuming y contains integer labels\n",
    "y = to_categorical(y, num_classes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "\n",
    "# Now proceed with the train_test_split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=1, stratify=y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9400, 64, 64, 3) (1175, 64, 64, 3) (1175, 64, 64, 3)\n",
      "(9400, 7) (1175, 7) (1175, 7)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "print(y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, Dropout, Conv2D, AvgPool2D, Flatten, LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-26 14:28:09.446009: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-26 14:28:10.789461: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-26 14:28:10.789591: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-26 14:28:10.790977: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-26 14:28:10.791087: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-26 14:28:10.791163: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-26 14:28:10.873620: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-26 14:28:10.873738: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-26 14:28:10.873821: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-26 14:28:10.873879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11416 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 64, 64, 32)        1568      \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 64, 64, 32)        128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 64, 64, 32)        0         \n",
      "                                                                 \n",
      " average_pooling2d (Average  (None, 16, 16, 32)        0         \n",
      " Pooling2D)                                                      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 16, 16, 16)        8208      \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 16, 16, 16)        64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 16, 16, 16)        0         \n",
      "                                                                 \n",
      " average_pooling2d_1 (Avera  (None, 4, 4, 16)          0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 4, 4, 8)           2056      \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 4, 4, 8)           32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 4, 4, 8)           0         \n",
      "                                                                 \n",
      " average_pooling2d_2 (Avera  (None, 1, 1, 8)           0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 8)                 0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               1152      \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 128)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7)                 903       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 7)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14623 (57.12 KB)\n",
      "Trainable params: 14255 (55.68 KB)\n",
      "Non-trainable params: 368 (1.44 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Input(X_train.shape[1:]),\n",
    "    \n",
    "    Conv2D(filters=32, kernel_size=(4,4), strides=(1,1), padding='same'),\n",
    "    BatchNormalization(),\n",
    "    LeakyReLU(alpha=0.01),\n",
    "    AvgPool2D(pool_size=(4,4)),\n",
    "    \n",
    "    Conv2D(filters=16, kernel_size=(4,4), strides=(1,1), padding='same'),\n",
    "    BatchNormalization(),\n",
    "    LeakyReLU(alpha=0.01),\n",
    "    AvgPool2D(pool_size=(4,4)),\n",
    "    \n",
    "    Conv2D(filters=8, kernel_size=(4,4), strides=(1,1), padding='same'),\n",
    "    BatchNormalization(),\n",
    "    LeakyReLU(alpha=0.01),\n",
    "    AvgPool2D(pool_size=(4,4)),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(128),\n",
    "    BatchNormalization(),\n",
    "    LeakyReLU(alpha=0.01),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Dense(7),\n",
    "    Activation('softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                               patience=10,\n",
    "                               verbose=1,\n",
    "                               restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.2,\n",
    "                              patience=5,\n",
    "                              verbose=1,\n",
    "                              min_lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "start fitting\n",
      "Epoch 1/10000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-26 14:28:16.265049: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2023-12-26 14:28:19.875587: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fa54967f830 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-26 14:28:19.875602: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2023-12-26 14:28:20.032481: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1703568500.187270   41553 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294/294 [==============================] - 9s 6ms/step - loss: 1.1223 - accuracy: 0.6821 - val_loss: 1.1572 - val_accuracy: 0.6970 - lr: 0.0010\n",
      "Epoch 2/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 1.0159 - accuracy: 0.6974 - val_loss: 1.5063 - val_accuracy: 0.4077 - lr: 0.0010\n",
      "Epoch 3/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.9688 - accuracy: 0.7076 - val_loss: 1.4129 - val_accuracy: 0.4774 - lr: 0.0010\n",
      "Epoch 4/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.9255 - accuracy: 0.7170 - val_loss: 1.6519 - val_accuracy: 0.3702 - lr: 0.0010\n",
      "Epoch 5/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.9002 - accuracy: 0.7254 - val_loss: 1.2211 - val_accuracy: 0.7132 - lr: 0.0010\n",
      "Epoch 6/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.8774 - accuracy: 0.7282 - val_loss: 0.9117 - val_accuracy: 0.7404 - lr: 0.0010\n",
      "Epoch 7/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.8540 - accuracy: 0.7356 - val_loss: 0.9448 - val_accuracy: 0.7217 - lr: 0.0010\n",
      "Epoch 8/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.8364 - accuracy: 0.7368 - val_loss: 1.1005 - val_accuracy: 0.6043 - lr: 0.0010\n",
      "Epoch 9/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.8227 - accuracy: 0.7427 - val_loss: 1.1451 - val_accuracy: 0.6145 - lr: 0.0010\n",
      "Epoch 10/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.8059 - accuracy: 0.7450 - val_loss: 1.0053 - val_accuracy: 0.7319 - lr: 0.0010\n",
      "Epoch 11/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.7901 - accuracy: 0.7507 - val_loss: 1.0215 - val_accuracy: 0.7174 - lr: 0.0010\n",
      "Epoch 12/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.7814 - accuracy: 0.7528 - val_loss: 0.9242 - val_accuracy: 0.7132 - lr: 0.0010\n",
      "Epoch 13/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.7641 - accuracy: 0.7560 - val_loss: 1.0639 - val_accuracy: 0.6477 - lr: 0.0010\n",
      "Epoch 14/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.7553 - accuracy: 0.7605 - val_loss: 1.0291 - val_accuracy: 0.7132 - lr: 0.0010\n",
      "Epoch 15/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.7415 - accuracy: 0.7622 - val_loss: 1.2076 - val_accuracy: 0.6706 - lr: 0.0010\n",
      "Epoch 16/10000000\n",
      "290/294 [============================>.] - ETA: 0s - loss: 0.7294 - accuracy: 0.7642Restoring model weights from the end of the best epoch: 6.\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.7273 - accuracy: 0.7649 - val_loss: 1.1498 - val_accuracy: 0.5889 - lr: 0.0010\n",
      "Epoch 16: early stopping\n",
      "fitting done\n"
     ]
    }
   ],
   "source": [
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print('start fitting')\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10000000,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val,y_val),\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "print('fitting done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 1ms/step\n",
      "[0.08987872 0.12068116 0.10877512 0.00846811 0.48212352 0.01987756\n",
      " 0.17019583]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4, 4, 4, 4, 4, 4, 6, 4, 4, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(y_pred[0])\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_pred_labels[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7285106382978723\n",
      "Confusion Matrix:\n",
      " [[ 28   0   2   0  98   0   0]\n",
      " [  3   0   2   1  81   0   0]\n",
      " [  2   0  14   0  52   0   5]\n",
      " [  0   0   0   3  13   0   0]\n",
      " [ 13   0   2   2 797   0   4]\n",
      " [  0   0   3   0   4   0   0]\n",
      " [  0   0   0   0  32   0  14]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.22      0.32       128\n",
      "           1       0.00      0.00      0.00        87\n",
      "           2       0.61      0.19      0.29        73\n",
      "           3       0.50      0.19      0.27        16\n",
      "           4       0.74      0.97      0.84       818\n",
      "           5       0.00      0.00      0.00         7\n",
      "           6       0.61      0.30      0.41        46\n",
      "\n",
      "    accuracy                           0.73      1175\n",
      "   macro avg       0.44      0.27      0.30      1175\n",
      "weighted avg       0.65      0.73      0.66      1175\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bae/miniconda3/envs/tf/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/bae/miniconda3/envs/tf/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/bae/miniconda3/envs/tf/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "if y_test.ndim > 1:  # Check if y_test is one-hot encoded\n",
    "    y_test_int_labels = np.argmax(y_test, axis=1)\n",
    "else:\n",
    "    y_test_int_labels = y_test  # y_test is already in integer label format\n",
    "\n",
    "# Now calculate the accuracy\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_int_labels, y_pred_labels))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_int_labels, y_pred_labels))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_int_labels, y_pred_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('images64.npy')\n",
    "y = np.load('styles64.npy')\n",
    "# X and y are now numpy arrays containing your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9400, 12288) (1175, 12288) (1175, 12288)\n",
      "(9400,) (1175,) (1175,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.calibration import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Reshape X from (len(X), 64, 64, 3) to (len(X), 1, 12288)\n",
    "X = X.reshape(len(X),-1)  # -1 tells numpy to infer the dimension\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Now proceed with the train_test_split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=1, stratify=y_temp)\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "print(y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.91%\n",
      "Precision: 0.89\n",
      "Recall: 0.88\n",
      "F1 Score: 0.87\n",
      "Confusion Matrix:\n",
      "[[ 74   0   0   0  54   0   0]\n",
      " [  0  46   0   0  41   0   0]\n",
      " [  1   0  53   0  19   0   1]\n",
      " [  1   0   0  12   3   0   0]\n",
      " [  6   0   0   0 813   0   0]\n",
      " [  0   0   0   0   3   3   0]\n",
      " [  0   0   0   0  13   0  32]]\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "RFC = RandomForestClassifier()\n",
    "RFC.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_RFC = RFC.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy_RFC = accuracy_score(y_test, y_pred_RFC)\n",
    "print(f\"Accuracy: {accuracy_RFC * 100:.2f}%\")\n",
    "\n",
    "# Precision, Recall, and F1 Score\n",
    "precision_RFC = precision_score(y_test, y_pred_RFC, average='weighted')\n",
    "recall_RFC = recall_score(y_test, y_pred_RFC, average='weighted')\n",
    "f1_RFC = f1_score(y_test, y_pred_RFC, average='weighted')\n",
    "\n",
    "print(f\"Precision: {precision_RFC:.2f}\")\n",
    "print(f\"Recall: {recall_RFC:.2f}\")\n",
    "print(f\"F1 Score: {f1_RFC:.2f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_RFC = confusion_matrix(y_test, y_pred_RFC)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_RFC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.09%\n",
      "Precision: 0.89\n",
      "Recall: 0.88\n",
      "F1 Score: 0.87\n",
      "Confusion Matrix:\n",
      "[[ 75   0   0   0  53   0   0]\n",
      " [  1  46   0   0  40   0   0]\n",
      " [  1   0  53   0  20   0   0]\n",
      " [  0   0   0  12   4   0   0]\n",
      " [  5   0   0   0 814   0   0]\n",
      " [  0   0   0   0   3   3   0]\n",
      " [  0   0   0   0  13   0  32]]\n"
     ]
    }
   ],
   "source": [
    "# SVM Classifier\n",
    "SVM = SVC()\n",
    "\n",
    "# Random Forest\n",
    "SVM = RandomForestClassifier()\n",
    "SVM.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_SVM = SVM.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy_SVM = accuracy_score(y_test, y_pred_SVM)\n",
    "print(f\"Accuracy: {accuracy_SVM * 100:.2f}%\")\n",
    "\n",
    "# Precision, Recall, and F1 Score\n",
    "precision_SVM = precision_score(y_test, y_pred_SVM, average='weighted')\n",
    "recall_SVM = recall_score(y_test, y_pred_SVM, average='weighted')\n",
    "f1_SVM = f1_score(y_test, y_pred_SVM, average='weighted')\n",
    "\n",
    "print(f\"Precision: {precision_SVM:.2f}\")\n",
    "print(f\"Recall: {recall_SVM:.2f}\")\n",
    "print(f\"F1 Score: {f1_SVM:.2f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_SVM = confusion_matrix(y_test, y_pred_SVM)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_SVM)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.83%\n",
      "Precision: 0.89\n",
      "Recall: 0.88\n",
      "F1 Score: 0.87\n",
      "Confusion Matrix:\n",
      "[[ 75   0   0   0  53   0   0]\n",
      " [  0  47   0   1  39   0   0]\n",
      " [  1   0  53   0  19   0   1]\n",
      " [  1   0   0  12   3   0   0]\n",
      " [  7   1   1   0 810   0   0]\n",
      " [  0   0   0   0   3   3   0]\n",
      " [  0   0   0   0  13   0  32]]\n"
     ]
    }
   ],
   "source": [
    "# XGBClassifier\n",
    "XGB= xgb.XGBClassifier()\n",
    "\n",
    "XGB.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_XGB = XGB.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy_XGB = accuracy_score(y_test, y_pred_XGB)\n",
    "print(f\"Accuracy: {accuracy_XGB * 100:.2f}%\")\n",
    "\n",
    "# Precision, Recall, and F1 Score\n",
    "precision_XGB = precision_score(y_test, y_pred_XGB, average='weighted')\n",
    "recall_XGB = recall_score(y_test, y_pred_XGB, average='weighted')\n",
    "f1_XGB = f1_score(y_test, y_pred_XGB, average='weighted')\n",
    "\n",
    "print(f\"Precision: {precision_XGB:.2f}\")\n",
    "print(f\"Recall: {recall_XGB:.2f}\")\n",
    "print(f\"F1 Score: {f1_XGB:.2f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_XGB = confusion_matrix(y_test, y_pred_XGB)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_XGB)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 27.40%\n",
      "Precision: 0.62\n",
      "Recall: 0.27\n",
      "F1 Score: 0.34\n",
      "Confusion Matrix:\n",
      "[[ 22  23   4  10  19  11  39]\n",
      " [ 13  21   1  12  18   9  13]\n",
      " [  7  16  20   3   9   3  16]\n",
      " [  1   3   0   3   0   1   8]\n",
      " [ 71  98  51 103 223 108 165]\n",
      " [  0   0   0   0   0   3   3]\n",
      " [  5   2   1   2   4   1  30]]\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "GNB= GaussianNB()\n",
    "\n",
    "GNB.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_GNB = GNB.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy_GNB = accuracy_score(y_test, y_pred_GNB)\n",
    "print(f\"Accuracy: {accuracy_GNB * 100:.2f}%\")\n",
    "\n",
    "# Precision, Recall, and F1 Score\n",
    "precision_GNB = precision_score(y_test, y_pred_GNB, average='weighted')\n",
    "recall_GNB = recall_score(y_test, y_pred_GNB, average='weighted')\n",
    "f1_GNB = f1_score(y_test, y_pred_GNB, average='weighted')\n",
    "\n",
    "print(f\"Precision: {precision_GNB:.2f}\")\n",
    "print(f\"Recall: {recall_GNB:.2f}\")\n",
    "print(f\"F1 Score: {f1_GNB:.2f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_GNB = confusion_matrix(y_test, y_pred_GNB)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_GNB)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
