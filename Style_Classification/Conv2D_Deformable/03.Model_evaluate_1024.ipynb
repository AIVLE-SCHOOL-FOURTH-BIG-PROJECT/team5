{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast\n",
    "from torchvision.ops import DeformConv2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchsummary import summary\n",
    "from contextlib import redirect_stdout\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import json\n",
    "import gc\n",
    "import io\n",
    "import joblib\n",
    "\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data load success\n",
      "Data loader set\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "# Paths & indices\n",
    "h5_path = '/home/all/processed_data/image_torchtensor_1024.h5'  # Update with your path\n",
    "styles = np.load('/home/all/processed_data/styles_1024.npy', allow_pickle=True)  # Your styles data\n",
    "\n",
    "\n",
    "# Load one image to get the input shape\n",
    "with h5py.File(h5_path, 'r') as h5file:\n",
    "    one_file = h5file['images'][0:1]  # Load the first image\n",
    "\n",
    "# Not going to load X yet. because it is too big.\n",
    "# We are going to load X batch by batch when model.fit.\n",
    "\n",
    "le = joblib.load('label_encoder.joblib')\n",
    "\n",
    "y = le.transform(styles)\n",
    "\n",
    "# Convert the NumPy array of labels into a torch tensor\n",
    "# y_tensor = torch.from_numpy(y).long()  # Ensure it's a LongTensor for classification tasks\n",
    "\n",
    "\n",
    "# Assuming total number of images\n",
    "num_images = len(y)  # or len(combined_df)\n",
    "indices = np.arange(num_images)\n",
    "\n",
    "print('Data load success')\n",
    "\n",
    "# Split indices\n",
    "indices_train, indices_temp, y_train, y_temp = train_test_split(indices,y, test_size=0.2, random_state=1, stratify=y)\n",
    "indices_val, indices_test, y_val, y_test = train_test_split(indices_temp, y_temp, test_size=0.5, random_state=1, stratify=y_temp)\n",
    "\n",
    "np.save('indices_test.npy', np.array(indices_test))\n",
    "\n",
    "class H5Dataset(Dataset):\n",
    "    def __init__(self, h5_path, indices, styles):\n",
    "        self.h5_path = h5_path\n",
    "        self.indices = indices\n",
    "        self.styles = styles\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.h5_path, 'r') as h5file:\n",
    "            # Use the index to access the image and label\n",
    "            image = h5file['images'][self.indices[idx]]\n",
    "            styles = self.styles[self.indices[idx]]\n",
    "            return torch.from_numpy(image).float(), torch.tensor(styles).long()\n",
    "\n",
    "# Load your data and labels\n",
    "train_data = H5Dataset(h5_path, indices_train, y)\n",
    "val_data = H5Dataset(h5_path, indices_val, y)\n",
    "test_data = H5Dataset(h5_path, indices_test, y)\n",
    "\n",
    "batch_size = 3  # Define your batch size\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "print('Data loader set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for Offset. making offset as a trainable variable, and keep the already trained offset consistent within next batch.\n",
    "class OffsetPredictor(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super(OffsetPredictor, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# Handmade Conv2D Model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.offset_predictor1 = OffsetPredictor(one_file.shape[1], 2*4*4, kernel_size=4, stride=1, padding=1)  # For a 4x4 kernel\n",
    "        self.offset_predictor2 = OffsetPredictor(128, 2*4*4, kernel_size=4, stride=1, padding=1)\n",
    "        self.offset_predictor3 = OffsetPredictor(96, 2*4*4, kernel_size=4, stride=1, padding=1)\n",
    "        self.offset_predictor4 = OffsetPredictor(64, 2*4*4, kernel_size=4, stride=1, padding=1)\n",
    "        self.offset_predictor5 = OffsetPredictor(32, 2*4*4, kernel_size=4, stride=1, padding=1)\n",
    "        \n",
    "        # Deformable Convolution layers\n",
    "        \n",
    "        # 3 -> 128\n",
    "        self.deform_conv2d1 = DeformConv2d(in_channels=one_file.shape[1], out_channels=128, kernel_size=4, stride=1, padding=1)  # padding=1 for 'same'\n",
    "        self.batchnorm2d1   = nn.BatchNorm2d(128)\n",
    "        self.leakyrelu1     = nn.LeakyReLU(0.01)\n",
    "        self.dropout2d1     = nn.Dropout2d(p=0.2)\n",
    "        self.avgpool2d1     = nn.AvgPool2d(kernel_size=2)\n",
    "        \n",
    "        # 128 -> 96\n",
    "        self.deform_conv2d2 = DeformConv2d(in_channels=128, out_channels=96, kernel_size=4, stride=1, padding=1)\n",
    "        self.batchnorm2d2   = nn.BatchNorm2d(96)\n",
    "        self.leakyrelu2     = nn.LeakyReLU(0.01)\n",
    "        self.dropout2d2     = nn.Dropout2d(p=0.2)\n",
    "        self.avgpool2d2     = nn.AvgPool2d(kernel_size=2)\n",
    "\n",
    "        # 96 -> 64\n",
    "        self.deform_conv2d3 = DeformConv2d(in_channels=96, out_channels=64, kernel_size=4, stride=1, padding=1)\n",
    "        self.batchnorm2d3   = nn.BatchNorm2d(64)\n",
    "        self.leakyrelu3     = nn.LeakyReLU(0.01)\n",
    "        self.dropout2d3     = nn.Dropout2d(p=0.2)\n",
    "        self.avgpool2d3     = nn.AvgPool2d(kernel_size=2)\n",
    "        \n",
    "        # 64 -> 32\n",
    "        self.deform_conv2d4 = DeformConv2d(in_channels=64, out_channels=32, kernel_size=4, stride=1, padding=1)\n",
    "        self.batchnorm2d4   = nn.BatchNorm2d(32)\n",
    "        self.leakyrelu4     = nn.LeakyReLU(0.01)\n",
    "        self.dropout2d4     = nn.Dropout2d(p=0.2)\n",
    "        self.avgpool2d4     = nn.AvgPool2d(kernel_size=2)\n",
    "\n",
    "        # 32 -> 16\n",
    "        self.deform_conv2d5 = DeformConv2d(in_channels=32, out_channels=16, kernel_size=4, stride=1, padding=1)\n",
    "        self.batchnorm2d5   = nn.BatchNorm2d(16)\n",
    "        self.leakyrelu5     = nn.LeakyReLU(0.01)\n",
    "        self.dropout2d5     = nn.Dropout2d(p=0.2)\n",
    "        self.avgpool2d5     = nn.AvgPool2d(kernel_size=2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.flatten      = nn.Flatten()\n",
    "        self.dropout1d1   = nn.Dropout(0.3)\n",
    "        self.linear1      = nn.Linear(15376,4096)  # 16384 개 나와야함 / padding 뭔가 이상해서 15376개 나옴\n",
    "        self.batchnorm1d1 = nn.BatchNorm1d(4096)\n",
    "        self.leakyrelu6   = nn.LeakyReLU(0.01)\n",
    "        self.linear2      = nn.Linear(4096,512)\n",
    "        self.batchnorm1d2 = nn.BatchNorm1d(512)\n",
    "        self.leakyrelu7   = nn.LeakyReLU(0.01)\n",
    "        self.linear3      = nn.Linear(512,64)\n",
    "        self.batchnorm1d3 = nn.BatchNorm1d(64)\n",
    "        self.leakyrelu8   = nn.LeakyReLU(0.01)\n",
    "        self.linear4      = nn.Linear(64,7)\n",
    "                          # nn.Softmax(dim=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Predict offsets for the first deformable convolution layer\n",
    "        offset1 = self.offset_predictor1(x)        \n",
    "        x = self.deform_conv2d1(x, offset1)\n",
    "        x = self.batchnorm2d1(x)\n",
    "        x = self.leakyrelu1(x)\n",
    "        x = self.dropout2d1(x)\n",
    "        x = self.avgpool2d1(x)\n",
    "        \n",
    "        offset2 = self.offset_predictor2(x)        \n",
    "        x = self.deform_conv2d2(x, offset2)\n",
    "        x = self.batchnorm2d2(x)\n",
    "        x = self.leakyrelu2(x)\n",
    "        x = self.dropout2d2(x)\n",
    "        x = self.avgpool2d2(x)\n",
    "        \n",
    "        offset3 = self.offset_predictor3(x)        \n",
    "        x = self.deform_conv2d3(x, offset3)\n",
    "        x = self.batchnorm2d3(x)\n",
    "        x = self.leakyrelu3(x)\n",
    "        x = self.dropout2d3(x)\n",
    "        x = self.avgpool2d3(x)\n",
    "        \n",
    "        offset4 = self.offset_predictor4(x)        \n",
    "        x = self.deform_conv2d4(x, offset4)\n",
    "        x = self.batchnorm2d4(x)\n",
    "        x = self.leakyrelu4(x)\n",
    "        x = self.dropout2d4(x)\n",
    "        x = self.avgpool2d4(x)\n",
    "\n",
    "        offset5 = self.offset_predictor5(x)        \n",
    "        x = self.deform_conv2d5(x, offset5)\n",
    "        x = self.batchnorm2d5(x)\n",
    "        x = self.leakyrelu5(x)\n",
    "        x = self.dropout2d5(x)\n",
    "        x = self.avgpool2d5(x)        \n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout1d1(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.batchnorm1d1(x)\n",
    "        x = self.leakyrelu6(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.batchnorm1d2(x)\n",
    "        x = self.leakyrelu7(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.batchnorm1d3(x)\n",
    "        x = self.leakyrelu8(x)\n",
    "        x = self.linear4(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.Model"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('best_model_state_dict.pth')\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Setting device to GPU if available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device('cpu')\n",
    "model = model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 1/391 [00:47<5:09:17, 47.58s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Move tensors to the same device as model\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 19\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Get predictions from the maximum value\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 83\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     80\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool2d1(x)\n\u001b[1;32m     82\u001b[0m offset2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffset_predictor2(x)        \n\u001b[0;32m---> 83\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeform_conv2d2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatchnorm2d2(x)\n\u001b[1;32m     85\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleakyrelu2(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torchvision/ops/deform_conv.py:170\u001b[0m, in \u001b[0;36mDeformConv2d.forward\u001b[0;34m(self, input, offset, mask)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, offset: Tensor, mask: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m        input (Tensor[batch_size, in_channels, in_height, in_width]): input tensor\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03m            masks to be applied for each position in the convolution kernel.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdeform_conv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdilation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torchvision/ops/deform_conv.py:92\u001b[0m, in \u001b[0;36mdeform_conv2d\u001b[0;34m(input, offset, weight, bias, stride, padding, dilation, mask)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_offset_grps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe shape of the offset tensor at dimension 1 is not valid. It should \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbe a multiple of 2 * weight.size[2] * weight.size[3].\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot offset.shape[1]=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moffset\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, while 2 * weight.size[2] * weight.size[3]=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m2\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39mweights_h\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39mweights_w\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     90\u001b[0m     )\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeform_conv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride_w\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_w\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdil_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdil_w\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_weight_grps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_offset_grps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/_ops.py:692\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;66;03m# is still callable from JIT\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Initialize necessary metrics\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# No need to track gradients for validation, which saves memory and computations\n",
    "with torch.no_grad():\n",
    "    # Wrap your loader with tqdm for a progress bar\n",
    "    pbar_test = tqdm(enumerate(test_loader), total=len(test_loader), desc=f\"Epoch 1/1\")\n",
    "    for i, (images, labels) in pbar_test:\n",
    "        with autocast():\n",
    "            # Move tensors to the same device as model\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Get predictions from the maximum value\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            y_pred.extend(predicted.cpu().numpy())  # Move back to cpu and convert to numpy\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            # Update progress bar\n",
    "            # pbar_test.set_postfix({'loss': running_loss / (i + 1)})\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy of the model on the test images: {accuracy}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bae/miniconda3/envs/torch/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.4863070696235053\n",
      "Recall: 0.6973572037510657\n",
      "F1 Score: 0.5730167681249339\n",
      "Confusion Matrix:\n",
      " [[  0   0   0   0 127   0   0]\n",
      " [  0   0   0   0  87   0   0]\n",
      " [  0   0   0   0  74   0   0]\n",
      " [  0   0   0   0  16   0   0]\n",
      " [  0   0   0   0 818   0   0]\n",
      " [  0   0   0   0   6   0   0]\n",
      " [  0   0   0   0  45   0   0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Calculating Precision, Recall, F1 Score, and Confusion Matrix\n",
    "precision = precision_score(y_true, y_pred, average='weighted')  # or other averaging method\n",
    "recall = recall_score(y_true, y_pred, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Printing the metrics\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Classic' 'Contemporary' 'Country' 'Minimalism' 'Modern' 'Unique' 'Urban']\n"
     ]
    }
   ],
   "source": [
    "# Assuming le is your LabelEncoder object and y_pred_labels are your predicted labels\n",
    "label_names = le.inverse_transform([0,1,2,3,4,5,6])\n",
    "\n",
    "# Now label_names will have the original string names of the labels\n",
    "print(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(train_loss) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# 1, 2, ... , num_epochs\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Creating subplots for loss and accuracy\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Plotting training and validation loss\u001b[39;00m\n\u001b[1;32m     15\u001b[0m ax[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mplot(epochs, train_loss, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "with open('history.json', 'r') as f:\n",
    "    history = json.load(f)\n",
    "\n",
    "# Extracting values for plotting\n",
    "train_loss = history['train_loss']\n",
    "val_loss = history['val_loss']\n",
    "train_accuracy = history['train_accuracy']\n",
    "val_accuracy = history['val_accuracy']\n",
    "epochs = range(1, len(train_loss) + 1)  # 1, 2, ... , num_epochs\n",
    "\n",
    "# Creating subplots for loss and accuracy\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plotting training and validation loss\n",
    "ax[0].plot(epochs, train_loss, 'r', label='Training loss')\n",
    "ax[0].plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "ax[0].set_title('Training and Validation Loss')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].legend()\n",
    "\n",
    "# Plotting training and validation accuracy\n",
    "ax[1].plot(epochs, train_accuracy, 'r', label='Training accuracy')\n",
    "ax[1].plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "ax[1].set_title('Training and Validation Accuracy')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "ax[1].legend()\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset_predictor1.conv.weight \n",
      "- over1: 0.00%, over10: 0.00%\n",
      "offset_predictor1.conv.bias \n",
      "- over1: 0.00%, over10: 0.00%\n",
      "offset_predictor2.conv.weight \n",
      "- over1: 0.00%, over10: 0.00%\n",
      "offset_predictor2.conv.bias \n",
      "- over1: 0.00%, over10: 0.00%\n",
      "offset_predictor3.conv.weight \n",
      "- over1: 0.00%, over10: 0.00%\n",
      "offset_predictor3.conv.bias \n",
      "- over1: 0.00%, over10: 0.00%\n",
      "offset_predictor4.conv.weight \n",
      "- over1: 0.00%, over10: 0.00%\n",
      "offset_predictor4.conv.bias \n",
      "- over1: 0.00%, over10: 0.00%\n",
      "offset_predictor5.conv.weight \n",
      "- over1: 0.00%, over10: 0.00%\n",
      "offset_predictor5.conv.bias \n",
      "- over1: 0.00%, over10: 0.00%\n",
      "deform_conv2d1.weight \n",
      "- over1: 0.00%, over10: 0.00%\n",
      "deform_conv2d1.bias \n",
      "- over1: 0.00%, over10: 0.00%\n",
      "batchnorm2d1.weight \n",
      "- over1: 48.44%, over10: 0.00%\n",
      "batchnorm2d1.bias \n",
      "- over1: 0.00%, over10: 0.00%\n",
      "batchnorm2d1.running_mean \n",
      "- over1: 34.38%, over10: 0.00%\n",
      "batchnorm2d1.running_var \n",
      "- over1: 3.12%, over10: 0.00%\n",
      "batchnorm2d1.num_batches_tracked \n",
      "- over1: 100.00%, over10: 100.00%\n",
      "deform_conv2d2.weight \n",
      "- over1: 0.00%, over10: 0.00%\n",
      "deform_conv2d2.bias \n",
      "- over1: 0.00%, over10: 0.00%\n",
      "batchnorm2d2.weight \n",
      "- over1: 45.83%, over10: 0.00%\n",
      "batchnorm2d2.bias \n",
      "- over1: 0.00%, over10: 0.00%\n",
      "batchnorm2d2.running_mean \n",
      "- over1: 88.54%, over10: 5.21%\n",
      "batchnorm2d2.running_var \n",
      "- over1: 100.00%, over10: 88.54%\n",
      "batchnorm2d2.num_batches_tracked \n",
      "- over1: 100.00%, over10: 100.00%\n",
      "deform_conv2d3.weight \n",
      "- over1: 0.00%, over10: 0.00%\n",
      "deform_conv2d3.bias \n",
      "- over1: 0.00%, over10: 0.00%\n",
      "batchnorm2d3.weight \n",
      "- over1: 46.88%, over10: 0.00%\n",
      "batchnorm2d3.bias \n",
      "- over1: 0.00%, over10: 0.00%\n",
      "batchnorm2d3.running_mean \n",
      "- over1: 64.06%, over10: 1.56%\n",
      "batchnorm2d3.running_var \n",
      "- over1: 100.00%, over10: 54.69%\n",
      "batchnorm2d3.num_batches_tracked \n",
      "- over1: 100.00%, over10: 100.00%\n",
      "deform_conv2d4.weight \n",
      "- over1: 0.00%, over10: 0.00%\n",
      "deform_conv2d4.bias \n",
      "- over1: 0.00%, over10: 0.00%\n",
      "batchnorm2d4.weight \n",
      "- over1: 50.00%, over10: 0.00%\n",
      "batchnorm2d4.bias \n",
      "- over1: 0.00%, over10: 0.00%\n",
      "batchnorm2d4.running_mean \n",
      "- over1: 62.50%, over10: 0.00%\n",
      "batchnorm2d4.running_var \n",
      "- over1: 100.00%, over10: 46.88%\n",
      "batchnorm2d4.num_batches_tracked \n",
      "- over1: 100.00%, over10: 100.00%\n",
      "deform_conv2d5.weight \n",
      "- over1: 0.00%, over10: 0.00%\n",
      "deform_conv2d5.bias \n",
      "- over1: 0.00%, over10: 0.00%\n",
      "batchnorm2d5.weight \n",
      "- over1: 37.50%, over10: 0.00%\n",
      "batchnorm2d5.bias \n",
      "- over1: 0.00%, over10: 0.00%\n",
      "batchnorm2d5.running_mean \n",
      "- over1: 50.00%, over10: 0.00%\n",
      "batchnorm2d5.running_var \n",
      "- over1: 100.00%, over10: 62.50%\n",
      "batchnorm2d5.num_batches_tracked \n",
      "- over1: 100.00%, over10: 100.00%\n",
      "linear1.weight \n",
      "- over1: 0.00%, over10: 0.00%\n",
      "linear1.bias \n",
      "- over1: 0.00%, over10: 0.00%\n",
      "batchnorm1d1.weight \n",
      "- over1: 49.24%, over10: 0.00%\n",
      "batchnorm1d1.bias \n",
      "- over1: 0.00%, over10: 0.00%\n",
      "batchnorm1d1.running_mean \n",
      "- over1: 72.12%, over10: 2.34%\n",
      "batchnorm1d1.running_var \n",
      "- over1: 99.98%, over10: 59.42%\n",
      "batchnorm1d1.num_batches_tracked \n",
      "- over1: 100.00%, over10: 100.00%\n",
      "linear2.weight \n",
      "- over1: 0.00%, over10: 0.00%\n",
      "linear2.bias \n",
      "- over1: 0.00%, over10: 0.00%\n",
      "batchnorm1d2.weight \n",
      "- over1: 48.63%, over10: 0.00%\n",
      "batchnorm1d2.bias \n",
      "- over1: 0.00%, over10: 0.00%\n",
      "batchnorm1d2.running_mean \n",
      "- over1: 75.78%, over10: 2.93%\n",
      "batchnorm1d2.running_var \n",
      "- over1: 99.61%, over10: 42.97%\n",
      "batchnorm1d2.num_batches_tracked \n",
      "- over1: 100.00%, over10: 100.00%\n",
      "linear3.weight \n",
      "- over1: 0.00%, over10: 0.00%\n",
      "linear3.bias \n",
      "- over1: 0.00%, over10: 0.00%\n",
      "batchnorm1d3.weight \n",
      "- over1: 1.56%, over10: 0.00%\n",
      "batchnorm1d3.bias \n",
      "- over1: 0.00%, over10: 0.00%\n",
      "batchnorm1d3.running_mean \n",
      "- over1: 53.12%, over10: 0.00%\n",
      "batchnorm1d3.running_var \n",
      "- over1: 75.00%, over10: 3.12%\n",
      "batchnorm1d3.num_batches_tracked \n",
      "- over1: 100.00%, over10: 100.00%\n",
      "linear4.weight \n",
      "- over1: 0.00%, over10: 0.00%\n",
      "linear4.bias \n",
      "- over1: 0.00%, over10: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# See weights and biases directly\n",
    "\n",
    "# Function to calculate percentages\n",
    "def calculate_percentage(tensor, threshold):\n",
    "    # Flatten the tensor to make the calculation easier\n",
    "    flattened_tensor = tensor.flatten()\n",
    "    # Calculate the number of elements greater than the threshold\n",
    "    count = torch.sum(flattened_tensor.abs() > threshold).item()\n",
    "    # Calculate the percentage\n",
    "    total_elements = flattened_tensor.numel()\n",
    "    percentage = 100.0 * count / total_elements\n",
    "    return percentage\n",
    "\n",
    "# Analysis\n",
    "for name, param in state_dict.items():\n",
    "    # Calculate percentages for each threshold\n",
    "    percent_over_1 = calculate_percentage(param, 1)\n",
    "    percent_over_10 = calculate_percentage(param, 10)\n",
    "    \n",
    "    print(f\"{name} \\n- over1: {percent_over_1:.2f}%, over10: {percent_over_10:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
