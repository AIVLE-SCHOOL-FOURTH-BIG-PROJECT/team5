{"cells":[{"cell_type":"markdown","metadata":{"id":"XA0-Z3mX6MBD"},"source":["## 로보 플로우 데이터셋"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25096,"status":"ok","timestamp":1703009097642,"user":{"displayName":"황교석","userId":"14795364109477648394"},"user_tz":-540},"id":"WMud3FqZeXyR","outputId":"c73727a2-ee8f-4e1b-9304-284e0602a555"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'roboflow'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mroboflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Roboflow\n\u001b[0;32m      3\u001b[0m rf \u001b[38;5;241m=\u001b[39m Roboflow(api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTBDNbDNNjBRxOoBGiu48\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m project \u001b[38;5;241m=\u001b[39m rf\u001b[38;5;241m.\u001b[39mworkspace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmokhamed-nagy-u69zl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mproject(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfurniture-detection-qiufc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'roboflow'"]}],"source":["from roboflow import Roboflow\n","\n","rf = Roboflow(api_key=\"TBDNbDNNjBRxOoBGiu48\")\n","project = rf.workspace(\"mokhamed-nagy-u69zl\").project(\"furniture-detection-qiufc\")\n","dataset = project.version(20).download(\"yolov8\")\n"]},{"cell_type":"markdown","metadata":{"id":"ujHPvgOO6OXU"},"source":["## 파일 관련 (이동, 갯수 계산, 사이즈 변환)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3OV70SyTQdHF"},"outputs":[],"source":["#전체 폴더 이동\n","# import os\n","# import shutil\n","\n","# source_folder = '/content/furniture-detection-20'  # 이동시킬 원본 폴더\n","# destination_folder = '/content/drive/MyDrive/Colab Notebooks/BigP/dataset'  # 파일을 이동할 대상 폴더\n","\n","# # 대상 폴더가 없으면 생성\n","# if not os.path.exists(destination_folder):\n","#     os.makedirs(destination_folder)\n","\n","# # 원본 폴더의 모든 내용(파일 및 하위 폴더)을 대상 폴더로 이동\n","# for item in os.listdir(source_folder):\n","#     source_path = os.path.join(source_folder, item)\n","#     destination_path = os.path.join(destination_folder, item)\n","\n","#     # 이동\n","#     shutil.move(source_path, destination_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-PfRlFXSJyqB"},"outputs":[],"source":["#파일 이동\n","import os\n","import shutil\n","\n","source_folder = '/content/runs/detect/train/weights'\n","destination_folder = '/content/drive/MyDrive/Colab Notebooks/BigP/weights/x_e100'\n","\n","# 대상 폴더가 없으면 생성\n","if not os.path.exists(destination_folder):\n","    os.makedirs(destination_folder)\n","\n","for filename in os.listdir(source_folder):\n","    source_path = os.path.join(source_folder, filename)\n","    destination_path = os.path.join(destination_folder, filename)\n","    # 파일 이동\n","    shutil.move(source_path, destination_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":527,"status":"ok","timestamp":1703057919970,"user":{"displayName":"황교석","userId":"14795364109477648394"},"user_tz":-540},"id":"QE78ZnlvF9F6","outputId":"5a335220-eb51-46aa-de7f-9027012e7cb3"},"outputs":[{"name":"stdout","output_type":"stream","text":["images 파일 수: 891\n","labels 파일 수: 891\n"]}],"source":["#파일 수 계산\n","import os\n","def count_files(directory):\n","    return sum([len(files) for r, d, files in os.walk(directory)])\n","\n","aa = '/content/drive/MyDrive/Colab Notebooks/BigP/dataset/valid'  # 폴더 경로 지정\n","folder_path = f'{aa}/images'  # 폴더 경로 지정\n","folder_path2 = f'{aa}/labels'  # 폴더 경로 지정\n","print(f\"images 파일 수: {count_files(folder_path)}\")\n","print(f\"labels 파일 수: {count_files(folder_path2)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IX71LOpA6ejJ"},"outputs":[],"source":["# 이미지 크기 조절\n","from ultralytics import YOLO\n","from PIL import Image\n","import os, torch\n","\n","folder_path = '/content/'\n","for filename in os.listdir(folder_path):\n","    if filename.endswith('.png'):\n","        img_path = os.path.join(folder_path, filename)\n","        with Image.open(img_path) as img:\n","            # 이미지 크기 조절\n","            img_resized = img.resize((640, 640))\n","\n","            # 변경된 이미지 저장\n","            img_resized.save(img_path)"]},{"cell_type":"markdown","metadata":{"id":"6-gYk7CL6V3b"},"source":["## YOLOv8"]},{"cell_type":"markdown","metadata":{"id":"NlzpopVELe7h"},"source":["### large Model"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5501,"status":"ok","timestamp":1703052827804,"user":{"displayName":"황교석","userId":"14795364109477648394"},"user_tz":-540},"id":"Gzif-GdyeXyT","outputId":"5663bd2c-7acd-4ed9-f7a5-fd382f5e1153"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n","  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n","  2                  -1  3    279808  ultralytics.nn.modules.block.C2f             [128, 128, 3, True]           \n","  3                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n","  4                  -1  6   2101248  ultralytics.nn.modules.block.C2f             [256, 256, 6, True]           \n","  5                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n","  6                  -1  6   8396800  ultralytics.nn.modules.block.C2f             [512, 512, 6, True]           \n","  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n","  8                  -1  3   4461568  ultralytics.nn.modules.block.C2f             [512, 512, 3, True]           \n","  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n"," 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 12                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n"," 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 15                  -1  3   1247744  ultralytics.nn.modules.block.C2f             [768, 256, 3]                 \n"," 16                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n"," 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 18                  -1  3   4592640  ultralytics.nn.modules.block.C2f             [768, 512, 3]                 \n"," 19                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n"," 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 21                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n"," 22        [15, 18, 21]  1   5644480  ultralytics.nn.modules.head.Detect           [80, [256, 512, 512]]         \n","YOLOv8l summary: 365 layers, 43691520 parameters, 43691504 gradients, 165.7 GFLOPs\n","\n","Transferred 589/595 items from pretrained weights\n"]}],"source":["#라지 모델 불러오기\n","from ultralytics import YOLO\n","model_l = YOLO('yolov8l.yaml').load('C:\\\\Users\\\\user\\\\Desktop\\\\Detector\\\\best.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3QtBOvvFeXyT"},"outputs":[],"source":["#라지 모델 학습 / patience - 몇번까지 성능향상 없으면 조기중단\n","results_l = model_l.train(data='/content/drive/MyDrive/Colab Notebooks/BigP/dataset/data.yaml', epochs=1, imgsz=416)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7367,"status":"ok","timestamp":1703056988561,"user":{"displayName":"황교석","userId":"14795364109477648394"},"user_tz":-540},"id":"TcVSLzDNE41p","outputId":"a2875c32-993f-448f-834b-a3851c4a2aba"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","image 1/1 C:\\Users\\user\\Desktop\\Detector\\모던1.png: 576x640 (no detections), 768.1ms\n","Speed: 14.1ms preprocess, 768.1ms inference, 11.2ms postprocess per image at shape (1, 3, 576, 640)\n","Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n","\n","image 1/1 C:\\Users\\user\\Desktop\\Detector\\모던2.png: 416x640 (no detections), 465.8ms\n","Speed: 2.0ms preprocess, 465.8ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n","Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n","\n","image 1/1 C:\\Users\\user\\Desktop\\Detector\\모던3.png: 384x640 (no detections), 414.5ms\n","Speed: 1.5ms preprocess, 414.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n","\n","image 1/1 C:\\Users\\user\\Desktop\\Detector\\모던4.png: 416x640 (no detections), 520.0ms\n","Speed: 1.5ms preprocess, 520.0ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 640)\n","Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n","\n","image 1/1 C:\\Users\\user\\Desktop\\Detector\\모던5.png: 512x640 (no detections), 537.1ms\n","Speed: 2.0ms preprocess, 537.1ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n","Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n","\n","image 1/1 C:\\Users\\user\\Desktop\\Detector\\모던6.png: 448x640 (no detections), 518.2ms\n","Speed: 2.0ms preprocess, 518.2ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n","Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n"]}],"source":["fn = ['1','2','3','4','5','6']\n","for n in fn:\n","    model_l.predict(f\"C:\\\\Users\\\\user\\\\Desktop\\\\Detector\\\\모던{n}.png\", save=True, imgsz=640, conf=0.1)"]},{"cell_type":"markdown","metadata":{"id":"6_T1FgSVLsnx"},"source":["### extreme"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4565,"status":"ok","timestamp":1703118058034,"user":{"displayName":"황교석","userId":"14795364109477648394"},"user_tz":-540},"id":"OIvMWXtTLmxF","outputId":"bf943bca-350e-4199-f47e-f139daee82e0"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1      2320  ultralytics.nn.modules.conv.Conv             [3, 80, 3, 2]                 \n","  1                  -1  1    115520  ultralytics.nn.modules.conv.Conv             [80, 160, 3, 2]               \n","  2                  -1  3    436800  ultralytics.nn.modules.block.C2f             [160, 160, 3, True]           \n","  3                  -1  1    461440  ultralytics.nn.modules.conv.Conv             [160, 320, 3, 2]              \n","  4                  -1  6   3281920  ultralytics.nn.modules.block.C2f             [320, 320, 6, True]           \n","  5                  -1  1   1844480  ultralytics.nn.modules.conv.Conv             [320, 640, 3, 2]              \n","  6                  -1  6  13117440  ultralytics.nn.modules.block.C2f             [640, 640, 6, True]           \n","  7                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n","  8                  -1  3   6969600  ultralytics.nn.modules.block.C2f             [640, 640, 3, True]           \n","  9                  -1  1   1025920  ultralytics.nn.modules.block.SPPF            [640, 640, 5]                 \n"," 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 12                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n"," 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 15                  -1  3   1948800  ultralytics.nn.modules.block.C2f             [960, 320, 3]                 \n"," 16                  -1  1    922240  ultralytics.nn.modules.conv.Conv             [320, 320, 3, 2]              \n"," 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 18                  -1  3   7174400  ultralytics.nn.modules.block.C2f             [960, 640, 3]                 \n"," 19                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n"," 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 21                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n"," 22        [15, 18, 21]  1   8795008  ultralytics.nn.modules.head.Detect           [80, [320, 640, 640]]         \n","YOLOv8x summary: 365 layers, 68229648 parameters, 68229632 gradients, 258.5 GFLOPs\n","\n","Transferred 589/595 items from pretrained weights\n"]}],"source":["#익스트림 모델 불러오기\n","from ultralytics import YOLO\n","model_x = YOLO('yolov8x.yaml').load('/content/drive/MyDrive/Colab Notebooks/BigP/weights/x_e100/best.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pJ1BRh--eXyU","outputId":"2f46cf51-5f72-49c5-ee27-c8d048fccdfc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ultralytics YOLOv8.0.228 🚀 Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla V100-SXM2-16GB, 16151MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8x.yaml, data=/content/drive/MyDrive/Colab Notebooks/BigP/dataset/data.yaml, epochs=50, time=None, patience=10, batch=16, imgsz=416, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=None, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n","Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 755k/755k [00:00<00:00, 37.4MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Overriding model.yaml nc=80 with nc=25\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1      2320  ultralytics.nn.modules.conv.Conv             [3, 80, 3, 2]                 \n","  1                  -1  1    115520  ultralytics.nn.modules.conv.Conv             [80, 160, 3, 2]               \n","  2                  -1  3    436800  ultralytics.nn.modules.block.C2f             [160, 160, 3, True]           \n","  3                  -1  1    461440  ultralytics.nn.modules.conv.Conv             [160, 320, 3, 2]              \n","  4                  -1  6   3281920  ultralytics.nn.modules.block.C2f             [320, 320, 6, True]           \n","  5                  -1  1   1844480  ultralytics.nn.modules.conv.Conv             [320, 640, 3, 2]              \n","  6                  -1  6  13117440  ultralytics.nn.modules.block.C2f             [640, 640, 6, True]           \n","  7                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n","  8                  -1  3   6969600  ultralytics.nn.modules.block.C2f             [640, 640, 3, True]           \n","  9                  -1  1   1025920  ultralytics.nn.modules.block.SPPF            [640, 640, 5]                 \n"," 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 12                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n"," 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 15                  -1  3   1948800  ultralytics.nn.modules.block.C2f             [960, 320, 3]                 \n"," 16                  -1  1    922240  ultralytics.nn.modules.conv.Conv             [320, 320, 3, 2]              \n"," 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 18                  -1  3   7174400  ultralytics.nn.modules.block.C2f             [960, 640, 3]                 \n"," 19                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n"," 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 21                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n"," 22        [15, 18, 21]  1   8742043  ultralytics.nn.modules.head.Detect           [25, [320, 640, 640]]         \n","YOLOv8x summary: 365 layers, 68176683 parameters, 68176667 gradients, 258.3 GFLOPs\n","\n","Transferred 589/595 items from pretrained weights\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\n","Freezing layer 'model.22.dfl.conv.weight'\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n","Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to 'yolov8n.pt'...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 6.23M/6.23M [00:00<00:00, 196MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["WARNING ⚠️ NMS time limit 0.550s exceeded\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"]}],"source":["#익스트림 학습\n","results_x = model_x.train(data='/content/drive/MyDrive/Colab Notebooks/BigP/dataset/data.yaml', epochs=50, imgsz=416, patience=10, resume=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G2gglQkIOcXw"},"outputs":[],"source":["#디렉토리 내 모든 파일 이동\n","import os\n","import shutil\n","\n","source_folder = '/content/runs/detect/train/weights'\n","destination_folder = '/content/drive/MyDrive/Colab Notebooks/BigP/weights/x_e100'\n","\n","# 대상 폴더가 없으면 생성\n","if not os.path.exists(destination_folder):\n","    os.makedirs(destination_folder)\n","\n","for filename in os.listdir(source_folder):\n","    source_path = os.path.join(source_folder, filename)\n","    destination_path = os.path.join(destination_folder, filename)\n","    # 파일 이동\n","    shutil.move(source_path, destination_path)"]},{"cell_type":"markdown","metadata":{"id":"s0m792dx6G0T"},"source":["## 로보 플로우 모델"]},{"cell_type":"code","execution_count":65,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1788,"status":"ok","timestamp":1703053636689,"user":{"displayName":"황교석","userId":"14795364109477648394"},"user_tz":-540},"id":"EgJn7i4m53Ja","outputId":"a29afb6b-c7fb-40b1-c6cf-afc0b3b95b22"},"outputs":[{"name":"stdout","output_type":"stream","text":["loading Roboflow workspace...\n","loading Roboflow project...\n"]}],"source":["#로보 플로우 모델\n","from roboflow import Roboflow\n","rf = Roboflow(api_key=\"TBDNbDNNjBRxOoBGiu48\")\n","project = rf.workspace().project(\"furniture-detection-qiufc\")\n","model = project.version(20).model"]},{"cell_type":"code","execution_count":66,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6058,"status":"ok","timestamp":1703054737992,"user":{"displayName":"황교석","userId":"14795364109477648394"},"user_tz":-540},"id":"dTJTkSc_6B1Y","outputId":"55ca43b9-38a9-4280-bec3-3a6ae736236a"},"outputs":[],"source":["#로보 플로우 예측\n","import os\n","\n","imgJson = []\n","folder_path = 'C:\\\\Users\\\\user\\\\Desktop\\\\Detector\\\\yolo\\\\test_crawlingdata'\n","save_predict = 'C:\\\\Users\\\\user\\\\Desktop\\\\Detector\\\\yolo\\\\predict_img'\n","file_names = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n","\n","for fn in file_names[:2]:\n","    model.predict(f\"{folder_path}\\\\{fn}\", confidence=50, overlap=20).save(f'{save_predict}\\\\pred_{fn}')\n","    imgJson.append(model.predict(f\"{folder_path}\\\\{fn}\", confidence=50, overlap=20).json())"]},{"cell_type":"markdown","metadata":{},"source":["## 이미지 크롭"]},{"cell_type":"markdown","metadata":{},"source":["#### 박스만 남기기"]},{"cell_type":"code","execution_count":72,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'x': 719, 'y': 289, 'width': 1438, 'height': 578, 'confidence': 0.8438215255737305, 'class': 'Sofa', 'class_id': 18, 'image_path': 'C:\\\\Users\\\\user\\\\Desktop\\\\Detector\\\\yolo\\\\test_crawlingdata\\\\CasaAndreaMilanoModernVelvetFabricSectionalSofaLShapeCouchwithExtraWideChaiseLounge.jpg', 'prediction_type': 'ObjectDetectionModel'}\n","[0, 0, 1438, 578] 0 0 1438 578\n","Saved cropped image\n","{'x': 726, 'y': 892, 'width': 1453, 'height': 868, 'confidence': 0.8170931339263916, 'class': 'Sofa', 'class_id': 18, 'image_path': 'C:\\\\Users\\\\user\\\\Desktop\\\\Detector\\\\yolo\\\\test_crawlingdata\\\\CHITAOversizedModularSectionalFabricSofaSetExtraLargeLShapedCouchwithReversibleChaiseModularSectionalCouchinchWidthSeatModularSofawithStorageOttamanLinen.jpg', 'prediction_type': 'ObjectDetectionModel'}\n","{'x': 1393, 'y': 407, 'width': 212, 'height': 814, 'confidence': 0.7027231454849243, 'class': 'Window', 'class_id': 23, 'image_path': 'C:\\\\Users\\\\user\\\\Desktop\\\\Detector\\\\yolo\\\\test_crawlingdata\\\\CHITAOversizedModularSectionalFabricSofaSetExtraLargeLShapedCouchwithReversibleChaiseModularSectionalCouchinchWidthSeatModularSofawithStorageOttamanLinen.jpg', 'prediction_type': 'ObjectDetectionModel'}\n","{'x': 167, 'y': 326, 'width': 292, 'height': 371, 'confidence': 0.6054579019546509, 'class': 'Frame', 'class_id': 10, 'image_path': 'C:\\\\Users\\\\user\\\\Desktop\\\\Detector\\\\yolo\\\\test_crawlingdata\\\\CHITAOversizedModularSectionalFabricSofaSetExtraLargeLShapedCouchwithReversibleChaiseModularSectionalCouchinchWidthSeatModularSofawithStorageOttamanLinen.jpg', 'prediction_type': 'ObjectDetectionModel'}\n","{'x': 555, 'y': 292, 'width': 274, 'height': 331, 'confidence': 0.5552124381065369, 'class': 'Frame', 'class_id': 10, 'image_path': 'C:\\\\Users\\\\user\\\\Desktop\\\\Detector\\\\yolo\\\\test_crawlingdata\\\\CHITAOversizedModularSectionalFabricSofaSetExtraLargeLShapedCouchwithReversibleChaiseModularSectionalCouchinchWidthSeatModularSofawithStorageOttamanLinen.jpg', 'prediction_type': 'ObjectDetectionModel'}\n","[0, 458, 1452, 1326] 0 458 1452 1326\n","Saved cropped image\n","[1287, 0, 1499, 814] 1287 0 1499 814\n","Saved cropped image\n","[21, 141, 313, 511] 21 141 313 511\n","Saved cropped image\n","[418, 127, 692, 457] 418 127 692 457\n","Saved cropped image\n"]},{"ename":"IndexError","evalue":"list index out of range","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[72], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(full_path)\n\u001b[0;32m      8\u001b[0m boxes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 10\u001b[0m NN \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mimgJson\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m N \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NN): \u001b[38;5;66;03m# N번째 박스\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(imgJson[n][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m][N])\n","\u001b[1;31mIndexError\u001b[0m: list index out of range"]}],"source":["import cv2\n","\n","full_path = f\"{folder_path}\\\\{file_names[n]}\"\n","save_crop = f'C:\\\\Users\\\\user\\\\Desktop\\\\Detector\\\\yolo\\\\crop_img'\n","\n","for n in range(len(file_names)):    \n","    img = cv2.imread(full_path)\n","    boxes = []\n","    \n","    NN = len(imgJson[n]['predictions'])\n","    for N in range(NN): # N번째 박스\n","        print(imgJson[n]['predictions'][N])\n","        xh = int(imgJson[n]['predictions'][N]['width']/2)\n","        yh = int(imgJson[n]['predictions'][N]['height']/2)\n","        x1 = imgJson[n]['predictions'][N]['x'] - xh\n","        y1 = imgJson[n]['predictions'][N]['y'] - yh\n","        x2 = imgJson[n]['predictions'][N]['x'] + xh\n","        y2 = imgJson[n]['predictions'][N]['y'] + yh\n","        boxes.append([x1, y1, x2, y2])\n","    \n","    for i, box in enumerate(boxes):\n","        x1, y1, x2, y2 = box\n","        print(box, x1, y1, x2, y2)\n","        # 바운딩 박스 영역 잘라내기\n","        cropped_img = img[y1:y2, x1:x2]\n","        cv2.imwrite(f'{save_crop}\\\\crop_{i}_{file_names[n]}.jpg', cropped_img)\n","        print(f\"Saved cropped image\")"]},{"cell_type":"markdown","metadata":{},"source":["#### 박스 제외 지우기"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'x': 719, 'y': 289, 'width': 1438, 'height': 578, 'confidence': 0.8438215255737305, 'class': 'Sofa', 'class_id': 18, 'image_path': 'C:\\\\Users\\\\user\\\\Desktop\\\\Detector\\\\yolo\\\\test_crawlingdata\\\\CasaAndreaMilanoModernVelvetFabricSectionalSofaLShapeCouchwithExtraWideChaiseLounge.jpg', 'prediction_type': 'ObjectDetectionModel'}\n","719 289 2157 867\n"]}],"source":["import cv2\n","\n","# # 이미지 로드\n","# img = cv2.imread(full_path)\n","\n","# # 객체 탐지 후, 바운딩 박스 좌표 예시\n","# boxes = [[x1, y1, x2, y2], ...]\n","\n","# # 이미지의 복사본 생성\n","# mask = np.zeros_like(img)\n","\n","# for box in boxes:\n","#     x1, y1, x2, y2 = box\n","#     # 바운딩 박스 영역을 복사본에 복사\n","#     mask[y1:y2, x1:x2] = img[y1:y2, x1:x2]\n","\n","# # 결과 이미지 보기\n","# cv2.imshow(\"Masked Image\", mask)\n","# cv2.waitKey(0)\n","# cv2.destroyAllWindows()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"V100","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":0}
